{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL:DR\n",
    "\n",
    "Here is testing of the code in a separate environment to extract images out of the ```zip``` archive and compress the images with their save in a separate ```zip``` archive with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Mediapipe face detection module\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Unzip and handle temporary folder\n",
    "\n",
    "The idea is that we don't want to keep all files in the RAM. Therefore, we create temporary folder for making transformations, form new ```zip``` archive out of changed data and then remove this temporary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_to_temp(archive_name: str, temp_directory: str=\"temp\"):\n",
    "    \"\"\"extract all images out of the zip archive into temporary directory.\n",
    "    IMPORTANT: consider that by default data is saved into 'temp' directory.\n",
    "\n",
    "    Args:\n",
    "        archive_name (str): name of archive to decompress\n",
    "        temp_directory (str): name of directory where to save images\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(archive_name, \"r\") as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                zip_ref.extract(file, path=temp_directory)\n",
    "    zip_ref.close()\n",
    "    \n",
    "\n",
    "def get_image_filepaths(target_directory: str) -> list:\n",
    "    \"\"\"get paths to all images in the directory\n",
    "\n",
    "    Args:\n",
    "        target_directory (str): directory where images are stored\n",
    "\n",
    "    Returns:\n",
    "        list: paths to images\n",
    "    \"\"\"\n",
    "    image_filepaths = []\n",
    "    for root, dirs, files in os.walk(target_directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                image_filepaths.append(os.path.join(root, file))\n",
    "    return image_filepaths\n",
    "\n",
    "def remove_temp_directory(temporary_directory: str):\n",
    "    \"\"\"remove temporary directory used to processing calculations\n",
    "\n",
    "    Args:\n",
    "        temporary_directory (str): name of directory to remove\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(temporary_directory, topdown=False):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))\n",
    "        for dir in dirs:\n",
    "            os.rmdir(os.path.join(root, dir))\n",
    "    os.rmdir(\"temp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Cropping faces out of the image\n",
    "\n",
    "First stage of reducing image size is to crop images in a way, where we keep only faces, removing all unrelated and non-required data. For this will be used ```mediapipe``` lib with ```FaceDetection``` module.\n",
    "\n",
    "Here will be also reviewed difference between image size in the original and result that will be achieved. Also, consider that in average around 10% of the records can be lost because mediapipe has possibility of not finding the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_zip_to_temp(\"images.zip\", temp_directory=\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_process = get_image_filepaths(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_to_process)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results demonstrated that the best efficiency is in case of cropping image and saving it as ```JPEG```. In case of making color palette there is no option of saving it as ```PNG```. The same goes for image quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = 0\n",
    "for image_path in images_to_process:\n",
    "    offset = 0.2\n",
    "    \n",
    "    #   first, we read image using openCV and convert it from BGR to RGB, because\n",
    "    # CV reads image as BGR\n",
    "    input_img = cv2.imread(image_path)\n",
    "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #   then we pass image to the face detector and crop image only if faces are found\n",
    "    results = face_detection.process(input_img)\n",
    "    if results.detections is not None:\n",
    "        processed_images += 1\n",
    "        for i, detection in enumerate(results.detections):\n",
    "            #   mediapipe marks face using \"relative\" coordinates, meaning that it's\n",
    "            # required to multiply relative coordinates with image shape. Here we detect\n",
    "            # X and Y coordinates of the initial point from which face starts and then width\n",
    "            # and height of the face in pixels\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            x, y, w, h = int(bbox.xmin * input_img.shape[1]), int(bbox.ymin * input_img.shape[0]), \\\n",
    "                        int(bbox.width * input_img.shape[1]), int(bbox.height * input_img.shape[0])\n",
    "            \n",
    "            #   now, there is an offset to add\n",
    "            x_offset, y_offset = int(w * offset), int(h * offset)\n",
    "            \n",
    "            #   considering face start point, its width and height, offsets to apply -\n",
    "            # calculate start point, end point and make sure that it's not going out\n",
    "            # of the image size\n",
    "            x = x - x_offset if x_offset < x else 0\n",
    "            y = y - y_offset if y_offset < y else 0\n",
    "            x_end = x + w + 2 * x_offset\n",
    "            y_end = y + h + 2 * y_offset \n",
    "            x_end = x_end if x_end < input_img.shape[1] else input_img.shape[1]\n",
    "            y_end = y_end if y_end < input_img.shape[0] else input_img.shape[0]\n",
    "            \n",
    "            #   extract face and convert face image from cv2 one into Pillow one,\n",
    "            # considering possible error of having incorrect image mode\n",
    "            face_img = input_img[y:y_end, x:x_end]           \n",
    "            face_img = Image.fromarray(face_img)\n",
    "            if face_img.mode != \"RGB\":\n",
    "                face_img = face_img.convert(\"RGB\")\n",
    "            \n",
    "            # Save the extracted face as a separate image file\n",
    "            if not os.path.exists(\"compressed\"):\n",
    "                os.makedirs(\"compressed\")\n",
    "            face_img.save(f\"{os.getcwd()}/compressed/face_{processed_images}.jpg\", \"JPEG\")\n",
    "            \n",
    "            # #   Now, considering that we saved the original face we want to see\n",
    "            # # if it is possible to compress image even more with minimal data loss.\n",
    "            # # So, first let's check how indexed version of the image will be smaller.\n",
    "            # # Convert the image to indexed color mode\n",
    "            # face_index_color_img = face_img.convert(\"P\", palette=Image.ADAPTIVE)\n",
    "            # face_index_color_img.save(f\"{image_path}_face_index_{i}.png\", \"PNG\")\n",
    "            \n",
    "            # #   The last check is to make quantized image, where colors will be reduced.\n",
    "            # # Therefore, less colors will mean smaller size, but colors won't be as accurate\n",
    "            # # as in the original image.\n",
    "            # face_index_color_quantized_img = face_img.quantize(colors=256)\n",
    "            # face_index_color_quantized_img.save(f\"{image_path}_face_index_quantized_{i}.png\", \"PNG\")\n",
    "            \n",
    "remove_temp_directory(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"test_name.zip\", \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "    for root, dirs, files in os.walk(\"compressed\"):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zip_file.write(file_path, os.path.relpath(file_path, \"compressed\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Making final function\n",
    "\n",
    "Idea is to take zip archive, extract its content, compress content and remove unnecessary data, then form a new archive with fixed package structure and the same name of archive with ```_compressed``` flag in the end of archive name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Mediapipe face detection module\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_to_temp(archive_name: str, temp_directory: str=\"temp\"):\n",
    "    \"\"\"extract all images out of the zip archive into temporary directory.\n",
    "    IMPORTANT: consider that by default data is saved into 'temp' directory.\n",
    "\n",
    "    Args:\n",
    "        archive_name (str): name of archive to decompress\n",
    "        temp_directory (str): name of directory where to save images\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(archive_name, \"r\") as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                zip_ref.extract(file, path=temp_directory)\n",
    "    zip_ref.close()\n",
    "    \n",
    "\n",
    "def get_image_filepaths(target_directory: str) -> list:\n",
    "    \"\"\"get paths to all images in the directory\n",
    "\n",
    "    Args:\n",
    "        target_directory (str): directory where images are stored\n",
    "\n",
    "    Returns:\n",
    "        list: paths to images\n",
    "    \"\"\"\n",
    "    image_filepaths = []\n",
    "    for root, dirs, files in os.walk(target_directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                image_filepaths.append(os.path.join(root, file))\n",
    "    return image_filepaths\n",
    "\n",
    "def remove_temp_directory(temporary_directory: str):\n",
    "    \"\"\"remove temporary directory used to processing calculations\n",
    "\n",
    "    Args:\n",
    "        temporary_directory (str): name of directory to remove\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(temporary_directory, topdown=False):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))\n",
    "        for dir in dirs:\n",
    "            os.rmdir(os.path.join(root, dir))\n",
    "    os.rmdir(temporary_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_images_zip_archive(target_archive_path: str, temp_dir_name: str,\n",
    "                                compressed_temp_dir_name: str):\n",
    "    \"\"\"Take zip archive from given filename or path in 'target_archive_path',\n",
    "    extract images out of it into temporary directory that will be created with\n",
    "    'temp_dir_name' name, then from all images will be taken face and transformed\n",
    "    into JPEG format. Those optimized images are saved into 'compressed_temp_dir_name'\n",
    "    directory. At the final stage, will be created archive with the name of original\n",
    "    one with added '_compressed' flag, after which all temporary folders and \n",
    "    original zip are removed \n",
    "\n",
    "    Args:\n",
    "        target_archive_path (str): name or path of archive to optimize\n",
    "        temp_dir_name (str): name of directory where images will be extracted from\n",
    "                original archive and that will be removed in the end of process\n",
    "        compressed_temp_dir_name (str): name of directory where compressed images\n",
    "                will be saved and that will be removed in the end of process.\n",
    "    \"\"\"\n",
    "    extract_zip_to_temp(target_archive_path, temp_directory=temp_dir_name)\n",
    "    images_to_process = get_image_filepaths(temp_dir_name)\n",
    "    \n",
    "    processed_images = 0\n",
    "    for image_path in images_to_process:\n",
    "        offset = 0.2\n",
    "        \n",
    "        #   first, we read image using openCV and convert it from BGR to RGB, because\n",
    "        # CV reads image as BGR\n",
    "        input_img = cv2.imread(image_path)\n",
    "        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #   then we pass image to the face detector and crop image only if faces are found\n",
    "        results = face_detection.process(input_img)\n",
    "        if results.detections is not None:\n",
    "            processed_images += 1\n",
    "            for i, detection in enumerate(results.detections):\n",
    "                #   mediapipe marks face using \"relative\" coordinates, meaning that it's\n",
    "                # required to multiply relative coordinates with image shape. Here we detect\n",
    "                # X and Y coordinates of the initial point from which face starts and then width\n",
    "                # and height of the face in pixels\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x, y, w, h = int(bbox.xmin * input_img.shape[1]), int(bbox.ymin * input_img.shape[0]), \\\n",
    "                            int(bbox.width * input_img.shape[1]), int(bbox.height * input_img.shape[0])\n",
    "                \n",
    "                #   now, there is an offset to add\n",
    "                x_offset, y_offset = int(w * offset), int(h * offset)\n",
    "                \n",
    "                #   considering face start point, its width and height, offsets to apply -\n",
    "                # calculate start point, end point and make sure that it's not going out\n",
    "                # of the image size\n",
    "                x = x - x_offset if x_offset < x else 0\n",
    "                y = y - y_offset if y_offset < y else 0\n",
    "                x_end = x + w + 2 * x_offset\n",
    "                y_end = y + h + 2 * y_offset \n",
    "                x_end = x_end if x_end < input_img.shape[1] else input_img.shape[1]\n",
    "                y_end = y_end if y_end < input_img.shape[0] else input_img.shape[0]\n",
    "                \n",
    "                #   extract face and convert face image from cv2 one into Pillow one,\n",
    "                # considering possible error of having incorrect image mode\n",
    "                face_img = input_img[y:y_end, x:x_end]           \n",
    "                face_img = Image.fromarray(face_img)\n",
    "                if face_img.mode != \"RGB\":\n",
    "                    face_img = face_img.convert(\"RGB\")\n",
    "                \n",
    "                # Save the extracted face as a separate image file\n",
    "                if not os.path.exists(compressed_temp_dir_name):\n",
    "                    os.makedirs(compressed_temp_dir_name)\n",
    "                face_img.save(f\"{os.getcwd()}/{compressed_temp_dir_name}/face_{processed_images}.jpg\", \"JPEG\")\n",
    "    #   remove intermediate directory\n",
    "    remove_temp_directory(temp_dir_name)\n",
    "    \n",
    "    #   we want the name of original archive with flag that it's compressed\n",
    "    base_name, extension = os.path.splitext(target_archive_path)\n",
    "    new_base_name = base_name + \"_compressed\"\n",
    "    new_file_name = new_base_name + extension\n",
    "    \n",
    "    #   make new archive with contents of the directory containing compressed images\n",
    "    with zipfile.ZipFile(new_file_name, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        for root, dirs, files in os.walk(compressed_temp_dir_name):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zip_file.write(file_path, os.path.relpath(file_path, compressed_temp_dir_name))\n",
    "    \n",
    "    #   final removal of intermediate directory with compressed image and original zip archive\n",
    "    remove_temp_directory(compressed_temp_dir_name)\n",
    "    if os.path.exists(target_archive_path):\n",
    "        os.remove(target_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_images_zip_archive(\"images.zip\", \"temporary\", \"compressed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unexpected continue of work\n",
    "\n",
    "As it was discovered, Mediapipe runs strangely on a server and is not able to be working (some deal with codecs) but the CV models work properly. Therefore, here we'll concentrate more on making optimized face cropping technique using openCV module.\n",
    "\n",
    "Generally there are two main models used for face detection:\n",
    "\n",
    "1. Haar cascade - well it is mostly based on understanding specific shapes and forms to represent the face of a person, what type of lines for human face. Computationally intensive and requires a lot of time\n",
    "2. LBP classifier - faster, because it is based more on performing analysis of the pixel groups. Less computations and faster, but efficiency is smaller.\n",
    "\n",
    "Here will be performed test of both detection approaches to see which one will have better coverage and compare speed of execution.\n",
    "\n",
    "# Haar Cascade approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#   loading Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "def extract_zip_to_temp(archive_name: str, temp_directory: str=\"temp\"):\n",
    "    \"\"\"extract all images out of the zip archive into temporary directory.\n",
    "    IMPORTANT: consider that by default data is saved into 'temp' directory.\n",
    "\n",
    "    Args:\n",
    "        archive_name (str): name of archive to decompress\n",
    "        temp_directory (str): name of directory where to save images\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(archive_name, \"r\") as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                zip_ref.extract(file, path=temp_directory)\n",
    "    zip_ref.close()\n",
    "    \n",
    "\n",
    "def get_image_filepaths(target_directory: str) -> list:\n",
    "    \"\"\"get paths to all images in the directory\n",
    "\n",
    "    Args:\n",
    "        target_directory (str): directory where images are stored\n",
    "\n",
    "    Returns:\n",
    "        list: paths to images\n",
    "    \"\"\"\n",
    "    image_filepaths = []\n",
    "    for root, dirs, files in os.walk(target_directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                image_filepaths.append(os.path.join(root, file))\n",
    "    return image_filepaths\n",
    "\n",
    "def remove_temp_directory(temporary_directory: str):\n",
    "    \"\"\"remove temporary directory used to processing calculations\n",
    "\n",
    "    Args:\n",
    "        temporary_directory (str): name of directory to remove\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(temporary_directory, topdown=False):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))\n",
    "        for dir in dirs:\n",
    "            os.rmdir(os.path.join(root, dir))\n",
    "    os.rmdir(temporary_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_images_zip_archive(target_archive_path: str, temp_dir_name: str,\n",
    "                                compressed_temp_dir_name: str):\n",
    "    \"\"\"Take zip archive from given filename or path in 'target_archive_path',\n",
    "    extract images out of it into temporary directory that will be created with\n",
    "    'temp_dir_name' name, then from all images will be taken face and transformed\n",
    "    into JPEG format. Those optimized images are saved into 'compressed_temp_dir_name'\n",
    "    directory. At the final stage, will be created archive with the name of original\n",
    "    one with added '_compressed' flag, after which all temporary folders and \n",
    "    original zip are removed \n",
    "\n",
    "    Args:\n",
    "        target_archive_path (str): name or path of archive to optimize\n",
    "        temp_dir_name (str): name of directory where images will be extracted from\n",
    "                original archive and that will be removed in the end of process\n",
    "        compressed_temp_dir_name (str): name of directory where compressed images\n",
    "                will be saved and that will be removed in the end of process.\n",
    "    \"\"\"\n",
    "    extract_zip_to_temp(target_archive_path, temp_directory=temp_dir_name)\n",
    "    images_to_process = get_image_filepaths(temp_dir_name)\n",
    "    \n",
    "    processed_images = 0\n",
    "    for image_path in images_to_process:\n",
    "        offset = 0.05\n",
    "        \n",
    "        #   first we reduce amount of required calculations by making grayscaled images\n",
    "        # instead of colored ones\n",
    "        input_img = cv2.imread(image_path)\n",
    "        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #   now we pass grayscaled image to the Haar Cascade and it will return list with\n",
    "        # all found face coordinates\n",
    "        faces = face_cascade.detectMultiScale(input_img, scaleFactor=1.1, minNeighbors=5)\n",
    "        for (x, y, w, h) in faces:            \n",
    "            processed_images += 1\n",
    "            #   considering found face, apply offset to save some additional face elements\n",
    "            x_offset, y_offset = int(w * offset), int(h * offset)\n",
    "            \n",
    "            #   considering face start point, its width and height, offsets to apply -\n",
    "            # calculate start point, end point and make sure that it's not going out\n",
    "            # of the image size\n",
    "            x = x - x_offset if x_offset < x else 0\n",
    "            y = y - y_offset if y_offset < y else 0\n",
    "            x_end = x + w + 2 * x_offset\n",
    "            y_end = y + h + 2 * y_offset \n",
    "            x_end = x_end if x_end < input_img.shape[1] else input_img.shape[1]\n",
    "            y_end = y_end if y_end < input_img.shape[0] else input_img.shape[0]\n",
    "            \n",
    "            #   extract face and convert face image from cv2 one into Pillow one,\n",
    "            # considering possible error of having incorrect image mode\n",
    "            face_img = input_img[y:y_end, x:x_end]           \n",
    "            face_img = Image.fromarray(face_img)\n",
    "            if face_img.mode != \"RGB\":\n",
    "                face_img = face_img.convert(\"RGB\")\n",
    "            \n",
    "            # Save the extracted face as a separate image file\n",
    "            if not os.path.exists(compressed_temp_dir_name):\n",
    "                os.makedirs(compressed_temp_dir_name)\n",
    "            face_img.save(f\"{os.getcwd()}/{compressed_temp_dir_name}/face_{processed_images}.jpg\", \"JPEG\")\n",
    "    # #   remove intermediate directory\n",
    "    # remove_temp_directory(temp_dir_name)\n",
    "    \n",
    "    # #   we want the name of original archive with flag that it's compressed\n",
    "    # base_name, extension = os.path.splitext(target_archive_path)\n",
    "    # new_base_name = base_name + \"_compressed\"\n",
    "    # new_file_name = new_base_name + extension\n",
    "    \n",
    "    # #   make new archive with contents of the directory containing compressed images\n",
    "    # with zipfile.ZipFile(new_file_name, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "    #     for root, dirs, files in os.walk(compressed_temp_dir_name):\n",
    "    #         for file in files:\n",
    "    #             file_path = os.path.join(root, file)\n",
    "    #             zip_file.write(file_path, os.path.relpath(file_path, compressed_temp_dir_name))\n",
    "    \n",
    "    # #   final removal of intermediate directory with compressed image and original zip archive\n",
    "    # remove_temp_directory(compressed_temp_dir_name)\n",
    "    # if os.path.exists(target_archive_path):\n",
    "    #     os.remove(target_archive_path)\n",
    "    \n",
    "    \n",
    "optimize_images_zip_archive(\"images.zip\", \"temporary\", \"compressed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result - out of 100 images with human face we got 102 faces, meaning that we got 2 false identification of the human face. Overall - great performance, but speed is around 4.5 seconds without removal of the temp folders and original zip.\n",
    "\n",
    "# LBP cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#   loading LBP Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(\"data/lbpcascade_frontalface.xml\")\n",
    "\n",
    "\n",
    "def extract_zip_to_temp(archive_name: str, temp_directory: str=\"temp\"):\n",
    "    \"\"\"extract all images out of the zip archive into temporary directory.\n",
    "    IMPORTANT: consider that by default data is saved into 'temp' directory.\n",
    "\n",
    "    Args:\n",
    "        archive_name (str): name of archive to decompress\n",
    "        temp_directory (str): name of directory where to save images\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(archive_name, \"r\") as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                zip_ref.extract(file, path=temp_directory)\n",
    "    zip_ref.close()\n",
    "    \n",
    "\n",
    "def get_image_filepaths(target_directory: str) -> list:\n",
    "    \"\"\"get paths to all images in the directory\n",
    "\n",
    "    Args:\n",
    "        target_directory (str): directory where images are stored\n",
    "\n",
    "    Returns:\n",
    "        list: paths to images\n",
    "    \"\"\"\n",
    "    image_filepaths = []\n",
    "    for root, dirs, files in os.walk(target_directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "                image_filepaths.append(os.path.join(root, file))\n",
    "    return image_filepaths\n",
    "\n",
    "def remove_temp_directory(temporary_directory: str):\n",
    "    \"\"\"remove temporary directory used to processing calculations\n",
    "\n",
    "    Args:\n",
    "        temporary_directory (str): name of directory to remove\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(temporary_directory, topdown=False):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))\n",
    "        for dir in dirs:\n",
    "            os.rmdir(os.path.join(root, dir))\n",
    "    os.rmdir(temporary_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_images_zip_archive(target_archive_path: str, temp_dir_name: str,\n",
    "                                compressed_temp_dir_name: str):\n",
    "    \"\"\"Take zip archive from given filename or path in 'target_archive_path',\n",
    "    extract images out of it into temporary directory that will be created with\n",
    "    'temp_dir_name' name, then from all images will be taken face and transformed\n",
    "    into JPEG format. Those optimized images are saved into 'compressed_temp_dir_name'\n",
    "    directory. At the final stage, will be created archive with the name of original\n",
    "    one with added '_compressed' flag, after which all temporary folders and \n",
    "    original zip are removed \n",
    "\n",
    "    Args:\n",
    "        target_archive_path (str): name or path of archive to optimize\n",
    "        temp_dir_name (str): name of directory where images will be extracted from\n",
    "                original archive and that will be removed in the end of process\n",
    "        compressed_temp_dir_name (str): name of directory where compressed images\n",
    "                will be saved and that will be removed in the end of process.\n",
    "    \"\"\"\n",
    "    extract_zip_to_temp(target_archive_path, temp_directory=temp_dir_name)\n",
    "    images_to_process = get_image_filepaths(temp_dir_name)\n",
    "    \n",
    "    processed_images = 0\n",
    "    for image_path in images_to_process:\n",
    "        offset = 0.05\n",
    "        \n",
    "        #   first we reduce amount of required calculations by making grayscaled images\n",
    "        # instead of colored ones\n",
    "        input_img = cv2.imread(image_path)\n",
    "        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #   now we pass grayscaled image to the Haar Cascade and it will return list with\n",
    "        # all found face coordinates\n",
    "        faces = face_cascade.detectMultiScale(input_img, scaleFactor=1.1, minNeighbors=10)\n",
    "        for (x, y, w, h) in faces:            \n",
    "            processed_images += 1\n",
    "            #   considering found face, apply offset to save some additional face elements\n",
    "            x_offset, y_offset = int(w * offset), int(h * offset)\n",
    "            \n",
    "            #   considering face start point, its width and height, offsets to apply -\n",
    "            # calculate start point, end point and make sure that it's not going out\n",
    "            # of the image size\n",
    "            x = x - x_offset if x_offset < x else 0\n",
    "            y = y - y_offset if y_offset < y else 0\n",
    "            x_end = x + w + 2 * x_offset\n",
    "            y_end = y + h + 2 * y_offset \n",
    "            x_end = x_end if x_end < input_img.shape[1] else input_img.shape[1]\n",
    "            y_end = y_end if y_end < input_img.shape[0] else input_img.shape[0]\n",
    "            \n",
    "            #   extract face and convert face image from cv2 one into Pillow one,\n",
    "            # considering possible error of having incorrect image mode\n",
    "            face_img = input_img[y:y_end, x:x_end]           \n",
    "            face_img = Image.fromarray(face_img)\n",
    "            if face_img.mode != \"RGB\":\n",
    "                face_img = face_img.convert(\"RGB\")\n",
    "            \n",
    "            # Save the extracted face as a separate image file\n",
    "            if not os.path.exists(compressed_temp_dir_name):\n",
    "                os.makedirs(compressed_temp_dir_name)\n",
    "            face_img.save(f\"{os.getcwd()}/{compressed_temp_dir_name}/face_{processed_images}.jpg\", \"JPEG\")\n",
    "    #   remove intermediate directory\n",
    "    remove_temp_directory(temp_dir_name)\n",
    "    \n",
    "    #   we want the name of original archive with flag that it's compressed\n",
    "    base_name, extension = os.path.splitext(target_archive_path)\n",
    "    new_base_name = base_name + \"_compressed\"\n",
    "    new_file_name = new_base_name + extension\n",
    "    \n",
    "    #   make new archive with contents of the directory containing compressed images\n",
    "    with zipfile.ZipFile(new_file_name, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        for root, dirs, files in os.walk(compressed_temp_dir_name):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zip_file.write(file_path, os.path.relpath(file_path, compressed_temp_dir_name))\n",
    "    \n",
    "    #   final removal of intermediate directory with compressed image and original zip archive\n",
    "    remove_temp_directory(compressed_temp_dir_name)\n",
    "    if os.path.exists(target_archive_path):\n",
    "        os.remove(target_archive_path)\n",
    "    \n",
    "    \n",
    "optimize_images_zip_archive(\"images.zip\", \"temporary\", \"compressed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBP classifier performs much better in terms of the speed compared to Haar Cascade. LBP dealed with image compression and face extraction in 2.5-2.7 seconds with making zip and removing intermediate folders. It was required to play a bit with amount of pixels on which it behaves rationally (like not detecting face in some shadow). In result - 10 neighbours is enough for it to work nice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuse_features_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
